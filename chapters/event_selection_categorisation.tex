\chapter{Event Categorisation}
\label{chap:event_select}

\newpage
\section{Overview and Objectives}
Once a set of candidate photons is assembled we tag and categorise events using extra final-state objects characteristic of particular Higgs production modes.
The objective of this tagging procedure is to enhance overall signficance, to construct categories of events with superior mass resolution, and to separate out the Higgs production modes for individual measurement. 


The event categorisation begins with a selection on the photon candidates with $p_{T}^{\gamma1}/m_{\gamma\gamma} > 1/3$, $p_{T}^{\gamma2}/m_{\gamma\gamma} > 1/4$, and $100 < m_{\gamma\gamma} < 180$\,GeV.
The use of mass-scaled $p_{T}$ here and in later machine learning models serves a dual purpose: firstly it avoids distortion of lower values in the $m_{\gamma\gamma}$ spectrum, secondly it avoids introducing mass bias from the simulated data during model trainings. 
There are then further requirements on the photons' supercluster pseudorapidities: both must have $|\eta| < 2.5$ to keep them in the fiducial region of the ECAL, and also must not be in the barrel-endcap transition region $1.44 < |\eta| < 1.57$ to ensure full containment of the electromagnetic showers. 


\subsection{The Diphoton BDT}
Selected diphoton candidates are then evaluated for signal-like kinematics and mass resolution by a BDT, the diphoton BDT, whose output score is used as a discriminating variable by the tags.
The input features of this BDT are the following:
\begin{itemize}[leftmargin=.5in,noitemsep]
    \item The mass-scaled transverse momentum $p^{\gamma}_{T}/m_{\gamma\gamma}$ for the leading and subleading photons
    \item The pseudorapidity $\eta$ for the leading and subleading photons
    \item The cosine of the azimuthal angle $\Delta\phi$ between the photons
    \item The score from the photon identification BDT for both photons
    \item The mass resolution estimate given the assumption that the correct vertex is selected, $\sigma^{RV}_{\gamma\gamma}/m_{\gamma\gamma}$
    \item The mass resolution estimate given the assumption that the incorrect vertex is selected, $\sigma^{WV}_{\gamma\gamma}/m_{\gamma\gamma}$
    \item The probability that the correct diphoton vertex has been selected, $p^{RV}$, estimated with the vertex probability BDT
\end{itemize}

The mass resolution in the right vertex case, $\sigma^{RV}_{\gamma\gamma}/m_{\gamma\gamma}$, is assumed to be completely dominated by the ECAL photon energy resolution, we can therefore neglect vertex uncertainty. The energy resolution for each photon can be approximated by a Gaussian distribution and combined in quadrature to give the following expression for mass resolution,
\begin{equation}
    \sigma^{RV}_{\gamma\gamma} = \frac{1}{2}\sqrt{(\sigma^{E}_{\gamma{1}}/E_{\gamma{1}})^2 + (\sigma^{E}_{\gamma{2}}/E_{\gamma{2}})^2}
\end{equation} 
where $\sigma^{E}_{\gamma{1}}/E_{\gamma{1}}, \sigma^{E}_{\gamma{2}}/E_{\gamma{2}}$ are the relative uncertainties on the photon energies for the leading and subleading photons respectively. 
In the wrong vertex case, $\sigma^{WV}_{\gamma\gamma}/m_{\gamma\gamma}$, we model the extra contribution to the mass resolution with an extra term. This term is assumed to be Gaussian in form, with a width qual to the extent in $z$ of the beamspot multiplied by $\sqrt{2}$. This extra term is then summed in quadrature with the mass resolution for the right vertex case,
\begin{equation}
    \sigma^{WV}_{\gamma\gamma} = \frac{1}{2}\sqrt{(\sigma^{RV}_{\gamma\gamma}/m_{\gamma\gamma})^2 + (\sigma^{V}_{\gamma\gamma}/m_{\gamma\gamma})^2}.
\end{equation} 


%(Training)
The diphoton BDT is trained on all four signal samples and the QCD, GJet and diphoton background samples. 
Each training event is weighted in proportion to its crosssection, its event weight and its expected mass resolution. 
When we weight events during training like this it can be considered to be a way of defining the `cost' of misclassifying a particular event. Higher weight events will have a higher associated misclassification cost and will therefore their correct classification be prioritised over lower weight events. 
Specifically, signal weight events are weighted as follows,
\begin{equation}
    w^{sig} = \frac{p^{RV}}{\sigma^{RV}_{\gamma\gamma}/m_{\gamma\gamma}} + \frac{1-p^{RV}}{\sigma^{WV}_{\gamma\gamma}/m_{\gamma\gamma}}.
\end{equation}
This scheme helps ensure that the diphoton BDT will assign a relatively high score to events with good expected mass resolution. 
The signal-flattened score distribution for all simulated signal and background samples, as well as data, is shown in the lefthand plot in Figure \ref{fig:event_categorisaton:diphoton_bdt}. 

%(Validation)
The performance of the diphoton BDT is validated in a \Zee control region where the normal diphoton selection has been applied, but the electron veto is inverted (Figure \ref{fig:event_categorisaton:diphoton_bdt} right). The Diphoton BDT output score has good agreement between data and simulation in the score region used by the event tagging.
\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.9\textwidth]{figures/event_selection/diphoton_BDT.pdf}
    \end{center}
    \caption{Diphoton BDT score distributions for simulated signal, background, and data (left); and the \Zee control region (right). 
             A transformation has been applied to the score distribution such that the total signal distribution is flat.}
        \label{fig:event_categorisaton:diphoton_bdt}
\end{figure}


\subsection{Tagging Scheme}
Tagging is implemented as a fall-through sequence where diphotons are offered to each tag in order of priority (Table \ref{tab:event_categorisaton:tag_sequence}). 
If a diphoton is not accepted by a tag it then passes to the next tag for consideration until the final `untagged' tag category. 
If the diphoton does not meet the criteria for this last tag it is discarded.
In the case of multiple tagged candidate diphotons in an event we select the one with the highest priority tag and category, if they are in the same category we choose the diphoton with the highest diphoton $p_{T}$.
\begin{table}[h!]
    \begin{tabular}{ l || l | l}
        Tag & Target Process & Structure \\
        \hline
        \ttH Leptonic      & \ttH with semi-leptonic top decays & Single category \\
        \ttH Hadronic      & \ttH with fully-hadronic top decays & Single category \\
        ZH Leptonic        & VH with leptonically-decaying Z boson & Single category \\
        WH Leptonic        & VH with leptonically-decaying W boson & Single category \\
        VH Leptonic Loose  & VH with leptonically-decaying W or Z boson & Single category \\
        VBF                & VBF with dijet in the final state & Three categories \\
        VH MET            & VH with significant amount of \MET & Single category \\
        VH Hadronic        & VH with hadronically-decaying W or Z boson & Single category \\
        Untagged           & Inclusive & Four categories \\
    \end{tabular}
    \caption{The \Hgg tag sequence}
    \label{tab:event_categorisaton:tag_sequence}
\end{table}







\section{Top Fusion Tagging}
In the \ttH production mode, a top-antitop pair is produced in association with the Higgs boson. The top quark immediately decays to a b quark and a W boson which will subsequently decay leptonically or hadronically. In the former (semi-leptonic) case there will be a bottom quark jet plus an associated lepton with \MET from the W decay. In the latter (fully-hadronic) case there will be a bottom quark jet plus two quark jets from the W decay to quarks (Figure \ref{fig:event_categorisaton:top_decays}). 
\begin{figure}[h!]
    \begin{center}
        \begin{tikzpicture}[baseline=(current bounding box.center)]
        \begin{feynman}
            \vertex (a) {$t$};
            \vertex [right=of a] (b);
            \vertex [above right=of b] (f1);
            \vertex [below right=of b] (f2) {$b$};
            \vertex [above right=of f1] (f3) {$(u,c,t)$};
            \vertex [below right=of f1] (f4) {$(\bar{d},\bar{s},\bar{b})$};
            \diagram* {
                (a) -- [fermion] (b) -- [boson, edge label=\(W^{+}\)] (f1),
                (b) -- [fermion] (f2),
                (f1) -- [fermion] (f3),
                (f4) -- [fermion] (f1),
            };
        \end{feynman}
        \end{tikzpicture}
        %
        \qquad
        \begin{tikzpicture}[baseline=(current bounding box.center)]
        \begin{feynman}
            \vertex (a) {$t$};
            \vertex [right=of a] (b);
            \vertex [above right=of b] (f1);
            \vertex [below right=of b] (f2) {$b$};
            \vertex [above right=of f1] (f3) {${(\nu_{e},\nu_{\mu},\nu_{\tau})}$};
            \vertex [below right=of f1] (f4) {${(\bar{e},\bar{\mu},\bar{\tau})}$};
            \diagram* {
                (a) -- [fermion] (b) -- [boson, edge label=\(W^{+}\)] (f1),
                (b) -- [fermion] (f2),
                (f1) -- [fermion] (f3),
                (f4) -- [fermion] (f1),
            };
        \end{feynman}
        \end{tikzpicture}
    \end{center}
    \caption{Top quark decay modes: a fully-hadronic decay (left) and a semi-leptonic decay (right).}
    \label{fig:event_categorisaton:top_decays}
\end{figure}

The top tags target these two decay modes: the leptonic tag searches for \ttH events where at least one top quark decays semi-leptonically, and the hadronic tag searches for \ttH events where both top quarks decay fully-hadronically. 

\subsection{\ttH Leptonic}
This tag uses a set of selections on kinematic properties of leptons and jets in the event. 
Leptons are required to pass selection requirements depending on their flavour
\begin{itemize}[leftmargin=.5in,noitemsep]
    \item Diphoton BDT score $> 0.11$ 
    \item At least one selected lepton with $p_{T} > 20$\,GeV
    \item All selected leptons are required to have an angular separation from a signal photon of $R(\ell,\gamma) > 0.35$
    \item $|m_{e\gamma} - m_{Z}| > 5$\,GeV (electrons only)
    \item A minimum of two jets in the event with $p_{T} > 25$\,GeV, $|\eta| < 2.4$, $R(j,\gamma) > 0.4$ and $R(j,\ell) > 0.4$
    \item At least one jet is tagged as b jet by the CSV tagger (medium requirement)
\end{itemize}


\subsection{\ttH Hadronic}
This tag uses a set of selections on kinematic properties of the jets in the event, as well as a dedicated BDT. The \ttH hadronic BDT is trained on the following input features:
\begin{itemize}[leftmargin=.5in,noitemsep]
    \item the number of jets with $p_{T} > 25$\,GeV,
    \item the $p_{T}$ of the leading jet,
    \item the two highest scores of the CSV b-tagger.
\end{itemize}
A selection on the BDT output score (Figure \ref{fig:event_categorisaton:tth_hadronic_bdt}) is optimised simultaneously on simulation with a selection on the diphoton BDT score to maximise expected precision on the signal strength of the \ttH production channel. 
\begin{figure}[h!]
    \includegraphics[width=0.49\textwidth]{figures/event_selection/Figure_006.pdf}
    \caption{Score distribution of the hadronic \ttH BDT. The blue lined histogram shows the distribution for the control region, the red filled histogram shows the score distribution for simulated signal, and the points show the score distribution of the data sideband regions ($m_{\gamma\gamma} < 115$\,GeV or $m_{\gamma\gamma} > 135$\,GeV).}
        \label{fig:event_categorisaton:tth_hadronic_bdt}
\end{figure}

A control region is constructed by selecting photon pairs where one passes the preselection and photon ID requirements, whilst the other has no preselection requirement and the photon ID is inverted.
These events are then weighted in $\eta$ and $p_T$ of the photons to reproduce the kinematic properties of the signal region.

The selection requirements of the \ttH Hadronic tag are as follows:
\begin{itemize}[leftmargin=.5in,noitemsep]
    \item $p_{T}/m_{\gamma\gamma} > 1/3$ and $1/4$ for leading and subleading photons respectively,
    \item diphoton BDT score $> 0.4$,
    \item no leptons that meet the criteria of the \ttH Leptonic tag
    \item a minimum of three jets in the event with $p_{T} > 25$\,GeV and $|\eta| < 2.4$,
    \item at least one jet is tagged as b jet by the CSV tagger (medium requirement),
    \item a \ttH Hadronic BDT score above 0.75.
\end{itemize}











\section{Associated Production Tagging}
In the associated production (VH) mode a W$^{\pm}$ or Z boson is produced in association with the Higgs boson. The VH tags target different vector bosons decaying in different ways which can manifest as leptons, jets or \MET in the event.
All of the leptonic VH tags are selection-based and have various isolation requirements to avoid contamination from Drell-Yan background processes.

\subsection{ZH Leptonic}
Targets Higgs production in association with a Z boson that subsequently decays leptonically with stringent requirements. The selection criteria are as follows:
\begin{itemize}[leftmargin=.5in,noitemsep]
    \item $p_{T}/m_{\gamma\gamma} > 3/8$ and $1/4$ for leading and subleading photons respectively,
    \item diphoton BDT score $> 0.11$,
    \item two same-flavour leptons with $p_T > 20$\,GeV and satisfying the same requirements as in the \ttH Leptonic tag
    \item $70 < m_{\ell\ell} < 110$\,GeV,
    \item $R(\gamma,e) > 1.0$, or $R(\gamma,\mu) > 0.5$,
    \item conversion electron veto: if an electron and a photon share a supercluser, the electron track must be well-separated from the supercluser centre ($R(SC,e) > 0.4$).
\end{itemize}


\subsection{WH Leptonic}
Targets Higgs production in association with a W$^{\pm}$ boson that subsequently decays leptonically with stringent requirements. The selection criteria are as follows:
\begin{itemize}[leftmargin=.5in,noitemsep]
    \item $p_{T}/m_{\gamma\gamma} > 3/8$ and $1/4$ for leading and subleading photons respectively,
    \item diphoton BDT score $> 0.28$,
    \item at minimum one lepton with $p_T > 20$\,GeV and satisfying the same requirements as in the \ttH Leptonic tag
    \item $R(\gamma,\ell) > 1.0$,
    \item \MET$> 45$\,GeV,
    \item a maximum of two jets each satisfying $p_T > 20$\,GeV, $|\eta| < 2.4$, $R(j,\ell) > 0.4$, $R(j,\gamma) > 0.4$,
    \item electron conversion veto as in the ZH Leptonic tag
\end{itemize}



\subsection{VH Leptonic Loose}
Targets Higgs production in association with either W$^{\pm}$ or Z which then decay leptonically. This tag uses a looser \MET selection of \MET$ < 45$\, GeV, with the rest of the selection being the same as WH Leptonic.

\subsection{VH MET}
Targets Higgs associated production with \MET from at least one missing lepton. The selection criteria are as follows:
\begin{itemize}[leftmargin=.5in,noitemsep]
    \item $p_{T}/m_{\gamma\gamma} > 3/8$ and $1/4$ for leading and subleading photons respectively,
    \item diphoton BDT score $> 0.79$,
    \item \MET$> 85$\,GeV,
    \item $|\Delta\phi(\gamma\gamma,E_{T}^{miss})| > 2.4$
\end{itemize}


\subsection{VH Hadronic}
Targets Higgs production in association with a W or Z boson that decays hadronically. The selection criteria are as follows:
\begin{itemize}[leftmargin=.5in,noitemsep]
    \item $p_{T}/m_{\gamma\gamma} > 1/2$ and $1/4$ for leading and subleading photons respectively, 
    \item diphoton BDT score $> 0.79$,
    \item a minimum of two jets with $p_T > 40$\,GeV and $|\eta| < 2.4$, $R(j,\gamma) > 0.4$,
    \item dijet invariant mass $60 < m_{jj} < 120$\,GeV,
    \item $|\mathrm{cos}{\theta^{*}}| < 0.5$, where $\theta^{*}$ is the difference in diphoton polar angle $\theta_{\gamma\gamma}$ between the diphoton-dijet centre-of-mass frame, and the lab frame.  
\end{itemize}













\section{VBF Tag}
The VBF production mode is characterised by its distinctive event topology and kinematics: two high-$p_{T}$ jets with large pseudorapidity separation and high invariant mass. Furthermore, the dijet substructure will also be distinctive with both jets originating from quarks, having colour connection to the proton remnant and possibly having correlations in structure between the two jets. 

Other production modes can also produce a Higgs boson in association with jets to produce a VBF-like final state. 
In particular, ggH can be a significant source of confusion due to its larger cross section and capacity to produce jets at next-to-leading order or from initial-state radiation. 
These dijets will mostly be from gluons, therefore targeting the jet substructure will be important in discriminating these production modes. 


%Couple sentences on the tag and the two models
The VBF tag targets the VBF production mode by exploiting the distinctive properties of VBF dijets. 
At the core of the VBF tag is a machine learning model which takes these distinctive properties as input features. 
The selection and category assignment of the tag is then based on the ouput of this model.
In this chapter we will explore two approaches:
\begin{itemize}[leftmargin=.5in,noitemsep]
    \item A tag based on two BDTs with engineered kinematic features. This is the approach used in the 2016 \Hgg analysis. 
    \item A tag based on a single dense convolutional neural network that receives jet structure information in the form of images in addition to engineered kinematic features. 
\end{itemize}
Both tags use the same event preselection, and produce scores used to define event categories which enhance the expected significance of the VBF channel and will be evaluated in the same way. The only difference will be the machine learning model that the tags are based around, and the extra image-based information in the dense CNN tag.  

%Problem formulation of the ML part
The problem formulation is the following: to separate VBF from SM background and ggH events using simulated data. 
Constraints are that the model must generalise to real data and must not introduce a bias to the diphoton mass. 
There are a few challenges associated with this problem:
\begin{itemize}[leftmargin=.5in,noitemsep]
    \item There is a severe class imballance where there are approximately seven times more examples of the background class than the signal
    \item The events are weighted, some events can be equivalent to multiple others
    \item Some events have negative weight and are needed for correct distribution shapes
    \item The total weight difference between the classes is very large and make the class imballance problem even worse
    \item The QCD background sample has very large weights and very few events. This causes the background distribution shapes to become very jagged. 
\end{itemize}



%Figures of merit
%Models
The model is evaluated using area under the ROC curve (AUROC), a performance measure of a binary classifier. 
This measure is chosen because it robust to class imballance and can easily be evaluated with weighted events.

%The Tag 
When developing the tag itself and its subcategories we use the approximate mean significance (REF) as the figure of merit. This is defined as
\begin{equation}
    \mathrm{AMS}_{2} = \sqrt{2\left( (s+b+b_{\mathrm{reg}})\log\left(1 + \frac{s}{b+b_{\mathrm{reg}}}\right) - s \right)},
\end{equation}
where $s$ is the total number of signal events, $b$ is the total number of background events, and $b_{\mathrm{reg}}$ is a regularisation term that reduces sensitivity to local optima. 
This is calculated by simulatneously fitting an exponential plus a double Gaussian function to the diphoton mass distribution. 
The background and signal event weights from an interval of two effective standard deviations around the peak are summed to produce $s$ and $b$, and to estimate $\mathrm{AMS}_{2}$. 







\subsection{Selections}
When a candidate diphoton is considered for VBF selection we apply additional requirements based on the jet content of the event. First requirements are applied on a per-jet basis, if there are more than two jets which meet these requirements the top two in $p_{T}$ are selected to form a dijet. Finally, a preselection based on dijet kinematics is applied the candate dijet. If it does not pass the event falls through to the untagged categories. 

\subsubsection{Jet Selection}
Jets are required to meet the criteria detailed in the previous chapter plus some additional requirements specific to the VBF tag.
Pileup jet ID (PUJID) uses a BDT classifier \ref{CMS-PAS-JME-13-005} that takes a collection of jet shape variables and produces a score for each jet. A collection of selections on this score are then applied for bins in $p_{T}$ and $\eta$ (Table \ref{tab:event_selection:tight_pujid}). In this analysis the tight working point is used as this gives the highest exptected significance for the VBF tag, and also leads to marked improvement in data/simulation agreement in $\eta$ in the \Zee plus jets control region.  
\begin{table}[h!]
    \begin{tabular}{ l || c | c | c | c }
         & $|\eta| < 2.5$ & $2.5 \leq |\eta| < 2.75$ & $2.75 \leq |\eta| < 3.0$ & $3.0 \leq |\eta| < 5.0$ \\
        \hline
        \hline
        $20 < p_{T} \leq 30$  & $0.69$ & $-0.35$ & $-0.26$ & $-0.21$ \\
        $30 < p_{T} \leq 50$  & $0.86$ & $-0.1$  & $-0.05$ & $-0.01$ \\
        $50 < p_{T} \leq 100$ & $0.95$ & $0.28$  & $0.31$  & $0.28$  \\
\end{tabular}
    \caption{Pileup jet ID cuts of the tight working point}
    \label{tab:event_selection:tight_pujid}
\end{table}
Furthermore there is a photon-jet isolation requirement where we require the jet to have $\Delta{R}(\gamma,j) > 0.4$ with both of the photons of the candidte diphoton, and we also require $|\eta_{j}| < 4.7$.







\subsubsection{Dijet Preselection}
Dijets are formed by selecting the two highest-$p_T$ jets in the event which pass the jet selection requirements. The highest-$p_T$ jet in the pair is referred to as the leading jet, and the other jet as the subleading jet. If there are fewer than two jets the event is rejected by the VBF tag and falls through to untagged. 
Candidate dijets are required to meet the following selection before being presented to the machine learning model:
\begin{itemize}[leftmargin=.5in,noitemsep]
    \item $p_{T}/m_{\gamma\gamma} > 1/3$ and $1/4$ for leading and subleading photon respectively,
    \item photon ID BDT score $> -0.2$ for both photons,
    \item dijet invariant mass $m_{jj} > 250$\,GeV,
    \item jet $p_{T} > 40$\,GeV and $> 30$\,GeV for the leading and subleading jets respectively,
%    \item absolute pseudorapidity $|\eta| < 4.7$ for both jets.
\end{itemize}

The criterion on the photon ID score is motivated by under-performance of the diphoton BDT in the VBF phase space. 
The diphoton BDT was trained over all signal, the bulk of which will consist of ggH where the diphoton is produced with no extra objects such as jets. 
In this phase space the transverse momentum of the diphoton system is highly-discriminating. 
This leads the diphoton BDT to assign a high score just on high values of diphoton $p_T$, and with lax requirements on photon ID. 
This in turn leads to under-performance in the VBf phase space where the $p_T$ spectrum is harder and low photon ID background events are given a high score. 








\subsection{BDT-based Model}
Once a candidate event passes the preselection it is presented to a machine learning model consisting of two BDTs in sequence: the dijet BDT and the combined BDT. 
The output classification score of this model will be used to define the subcategories of the tag selection. 

\subsubsection{Dijet BDT}
The purpose of the dijet BDT is to evaluate how VBF-like events are based on kinematic information from the dijet and the diphoton, and in particular to handle the rejection of ggH.
The BDT receives the following features which are chosen to minimise correlation with the diphoton mass:
\begin{itemize}[leftmargin=.5in,noitemsep]
    \item $p_{T}/m_{\gamma\gamma}$ for the leading and subleading photons
    \item $p_{T}^{j1}$ and $p_{T}^{j2}$, the transverse momenta of the leading and subleading jets respectively
    \item $m_{jj}$ the invariant mass of the dijet
    \item $\Delta\eta$ the pseudorapidity gap between the two jets
    \item $\mathrm{min}\Delta{R}(\gamma,j)$ the smallest angular separation between either of the diphoton photons and either of the dijet jets
    \item $|\Delta\phi_{\gamma\gamma{jj}}|$ the absolute azimuthal angular difference between the diphoton and dijet
    \item $|\Delta\phi_{jj}|$ the absolute azimuthal angular difference between the jets of the dijet
    \item $C_{\gamma\gamma}$ the diphoton centrality expressed as:
        \begin{equation}
            C_{\gamma\gamma} = \mathrm{exp}\left(-\frac{4}{(\eta_{j1} - \eta_{j2})^{2}}\left( \eta_{\gamma\gamma} - \frac{\eta_{j1} + \eta_{j2}}{2} \right)^{2}\right)
        \end{equation}
        where $\eta_{j1}$ and $\eta_{j2}$ are the pseudorapidities of the leading and subleading jets.
\end{itemize}
Their distributions with the VBF preselection applied are shown in Figure \ref{fig:event_categorisaton:dijet_bdt_features}.
\newpage
\begin{figure}[h!]
    \includegraphics[width=0.95\textwidth]{figures/event_selection/dijet_BDT_features_splitBG_PS.pdf}
    \caption{Dijet BDT feature distributions with the full VBF preselection. Distributions are all normalised to unity with solid red corresponding to VBF, blue line to ggH, and black line to SM background}
    \label{fig:event_categorisaton:dijet_bdt_features}
\end{figure}


This dijet BDT is trained on all simulated SM background samples with ggH included versus VBF. To increase the number of training examples we train with a loosened dijet preselection requirement where the $p_{T}^{\gamma}/m_{\gamma\gamma}$ are reduced to $1/4$ and $1/5$, the jet $p_T$ cuts are reduced by 10\,GeV, the dijet invariant mass cut is reduced to 100\,GeV.
The normalised score distributions for the classes and the ROC curves for each individual sample are shown in Figure \ref{fig:event_categorisaton:dijet_bdt_performance}.
These scores are then used as an input feature in the next BDT in the VBF tag: the combined BDT. 
\begin{figure}[h!]
        \includegraphics[width=0.75\textwidth]{figures/event_selection/dijet_BDT_PS.pdf}
    \caption{Dijet BDT performance. On the left are the output score distributions for VBF (red), ggH (blue) and SM background (black). On the right are the ROC curves for the dijet BDT split into the different samples. We note that the performance against ggH is lower than the other backgrounds.}
    \label{fig:event_categorisaton:dijet_bdt_performance}
\end{figure}









\subsubsection{Combined BDT}
The purpose of the combined BDT is to combine information from the diphoton BDT, and dijet BDT to produce the final discriminant score for defining VBF tag categories. 
Specifically, it takes the following input features
\begin{itemize}[leftmargin=.5in,noitemsep]
    \item diphoton BDT score
    \item dijet BDT score
    \item $p_{T}^{\gamma\gamma}/m_{\gamma\gamma}$, the mass-scaled transverse momentum of the diphoton
\end{itemize}
The distributions of these features with the full VBF preselection applied are shown in Figure \ref{fig:event_categorisaton:combined_bdt_features}.
\begin{figure}[h!]
    \includegraphics[width=0.98\textwidth]{figures/event_selection/combined_BDT_features_splitBG_PS.pdf}
    \caption{Combined BDT feature distributions with the full VBF preselection. Distributions are all normalised to unity with solid red corresponding to VBF, blue line to ggH, and black line to SM background}
    \label{fig:event_categorisaton:combined_bdt_features}
\end{figure}

The combined BDT is then trained with the SM background samples vs VBF. Gluon fusion is not included in this training as it is found to reduce the ability of this BDT to reject SM background, which is considered to be a higher priority than ggH rejection. 
The normalised combined score distributions for the classes and the ROC curves for each individual sample are shown in Figure \ref{fig:event_categorisaton:combined_bdt_performance}.
\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.75\textwidth]{figures/event_selection/combined_BDT_PS.pdf}
    \end{center}
    \caption{Combined BDT score distribution with the full VBF preselection. Distributions are all normalised to unity with solid red corresponding to VBF, blue line to ggH, and black line to SM background}
    \label{fig:event_categorisaton:combined_bdt_performance}
\end{figure}


\subsubsection{Model Interpretation}
We can interpret how features are used together by examining their joint distribution for highest and lowest-percentile scoring events. 
This is shown in Figure \ref{fig:event_categorisaton:bdt_based_vbf_tag_interpretation} where the solid colour shows the values averaged over each percentile, and the lines show the top five highest (or lowest) scoring events.
\begin{figure}[h!]
    \includegraphics[width=0.75\textwidth]{figures/event_selection/eng_feature_radar_BDT.pdf}
    \caption{Coloured regions correspond to mean values for top percentile (red) and bottom percentile (black) combined score events. 
             The top and bottom five scoring events are also shown by lines and dots.}
    \label{fig:event_categorisaton:bdt_based_vbf_tag_interpretation}
\end{figure}

%Some generic comments
We note that the discrimination power of the model is driven by the dijet angular variables, with the exception of $|\Delta\phi_{\gamma\gamma{jj}}|$ and the dijet mass.
Jet \pt is actually more of an indicator of background
Diphoton BDT score shows substantial overlap, probably from ggH but there could still be residual high-\pt SM background events.

%The most VBF
According to this model the most VBF-like event is...

%The most BG
According to this model the most background-like event is...




\subsection{Validation}
\subsubsection{\Zee Control Region}
To validate the VBF Tag we use the same \Zee control region as the other tags, but with the extra requirements of the VBF preselection. 
This control region is used for simulation/data comparison of both the features input to the BDTs and the BDT scores themselves (Figure \ref{fig:event_categorisation:zee_bdt_score_validation}). 
\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.49\textwidth]{figures/event_selection/dijet_BDT_zee_PS.pdf}
        \includegraphics[width=0.49\textwidth]{figures/event_selection/combined_BDT_zee_PS.pdf}
    \end{center}
    \caption{Data/Simulation comparison for dijet and combined BDT output scores.}
    \label{fig:event_categorisation:zee_bdt_score_validation}
\end{figure}
There is reasonable data-simulation agreement. The Kolmogorov-Smirnoff test statistic is calculated for these distributions 


These plots only show the marginal distributions of these variables, they do not show their joint behaviour. 
To examine data-simulation agreement of the joint distribution we train a BDT to discriminate between simulation and data events. 
If the discrimination power of the resulting model is high then the agreement is bad, if it is equivalent to guessing we can infer that the joint distributions match closely. 
We can then use selections on the BDT score to try to isolate regions of the joint distribution where there is disagreement. 

The results shown in Figure \ref{fig:event_categorisation:zee_bdt_validation} show that the score distributions for the simulation and data classes are very close. 
\begin{figure}[h!]
    \includegraphics[width=0.75\textwidth]{figures/event_selection/eng_feature_ROC_Zee_BDT.pdf}
    \includegraphics[width=0.6\textwidth]{figures/event_selection/eng_feature_radar_Zee_BDT.pdf}
    \caption{Joint distribution study with BDT in Zee control region}
    \label{fig:event_categorisation:zee_bdt_validation}
\end{figure}


\subsubsection{QCD Modelling Variations}
The accuracy of the simulation of QCD processes is a significant source of theoretical uncertainty and will have an impact on data-simulation agreement of the model. 
This will be especially apparent for features based on jet substructure.

To test for how such missmodelling affect the VBF tag we evaluate samples of VBF and ggH with variations on QCD simulation parameters.
These variations are...
We then evaluate how the performance of the model changes depending on the variant
ROC curves are shown in figure \ref{fig:event_categorisation:ps_variant_validation}
\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.75\textwidth]{figures/event_selection/psvar_ROCs_PS.pdf}
    \end{center}
    \caption{ROC curves for parton shower and pythia tune variations. The nominal performance is shown in black, 
             and the magenta lines show the upper and lower bounds of the envelope covering all the curves}
    \label{fig:event_categorisation:ps_variant_validation}
\end{figure}

We note that the change in performance is modest. This is expected because the VBF model only uses kinematic variables, and the main impact of these variations may be though the pileup mitigation. 







\subsection{Categorisation}
Once a candidate event has been preselected and evaluated by the model it is considered for inclusion in the VBF subcategories. 
These are defined as exclusive selections on the combined BDT output score and are chosen to maximise the $\mathrm{AMS}_2$ over all categories. 
If the combined score is not high enough for inclusion in the lowest subcategory it is rejected and is passed for consideration by lower-priority tags. 

The $\mathrm{AMS}_2$ is estimated for each category by constructing a diphoton mass histogram of all events in the subategory score range, we then fit an exponential function to the mass sidebands, and a double Gaussian in the signal region to the background-subtracted mass histogram, the parameters of these fits are then used as initial values for a simultaneuous signal-background fit. We then evaluate $s$ and $b$ in a region around the peak with width equal to two effective sigma either side estimated from the the simultaneous fit. Overall significance is then calculated as the sum in quadrature of the significance of each subcategory. 

The boundaries are optimised simultaneously for overall significance with a random search algorithm. The number of subcategories is chosen considering an increasingly larger number and performing a subcategory optimisation for each one. We stop when the improvement is less than one percent. This was found to be three. 

Approximate studies of the three VBF categories, their boundaries and their estimated performance is shown in Figure \ref{fig:event_categorisaton:bdt_mass_fits} and Table \ref{tab:event_selection:legacy_cats}.
\begin{figure}[h!]
    \includegraphics[width=1.0\textwidth]{figures/event_selection/BDT_mass_fits.pdf}
    \caption{Mass fits for estimating AMS$_2$}
    \label{fig:event_categorisaton:bdt_mass_fits}
\end{figure}
\begin{table}[h!]
    \begin{tabular}{ c || c | c | c | c | c | c }
        Category & Score Range & $\sigma_{\mathrm{eff}}$ & AMS$_2$ & $B_{\mathrm{ggH}}/(S+B_{\mathrm{ggH}})$ & $S/(S+B)$ & $S/S_{\mathrm{tot}}$ \\
        \hline
        0 & $[1, 0.957)$     & 1.4 &  2.16 & 0.20 & 0.37 & 0.25 \\
        1 & $[0.957, 0.902)$ & 1.2 &  1.0  & 0.34 & 0.17 & 0.13 \\
        2 & $[0.902, 0.553)$ & 2.0 &  0.69 & 0.53 & 0.04 & 0.30 \\
\end{tabular}
    \caption{Estimated category attributes for the BDT-based VBF tag}
    \label{tab:event_selection:legacy_cats}
\end{table}









\subsection{Single BDT Model}
%Really it should be a single step. This was developed on old technology with the old cuts. The ID cut does the job of the combined BDT now, and it just replicates the dijet BDT score. Conclusion is that the combined BDT is useless and the diphoton score should just be added to the set of dijet BDT input features. This gives the same performance and will be what we use to compare with the DCNN. 
The two-step structure of the VBF tag was first developed for the Higgs boson discovery on less performant software and selections. 
In particular, the 

The original train-test split of the two-step BDT was unavaiable. Evaluating over the entire sample will include training events and exaggerate the performance of the BDT-based tag. 
We train a single-step BDT and find that it equals the performance, a small increase in the two-step tag is possibly from the inclusion of training events. 
It is more of a fair test to compare this tag to the dense CNN because we can use the exact same training and holdout sets. 
\begin{figure}[h!]
    \includegraphics[width=0.99\textwidth]{figures/event_selection/dijet_BDT_PS_unw.pdf}
    \caption{Single BDT performance and comparison to the two step approach.}
    \label{fig:event_categorisation:single_BDT}
\end{figure}

















%=================================================
%---Dense CNN tag
%=================================================


\section{VBF Tag with a Dense Convolutional Neural Network}
Introduction, mention that the selection is the same, objectives and motivation for the new tag
2016 underperforms with ggH. ggH is a harder problem to solve. Need to go beyoned kinematics. Use jet structure.
A general way of representing this substructure is images. Dense CNNs are good at classifying images. 
We can use a dense CNN to form performant features without the need for human intervention. 

The objective of the new tag is to use jet images and a dense CNN to enhance VBF signal extraction, especially VBF vs Gluon Fusion discrimination. 
We will also introduce cost-sensitivity to the ML problem which has been done in an approximate unsystematic fashion before

Because the problem of building the model is so complex we'll use optimisation to do it for us. 

We'll also use a few techniques to figure out what sort of features it has build. Is it able to pick up on colour connection etc?





\subsection{Jet Images}
The spatial distribution and properties of a jet's constituent particles contain important discriminating information about the originating parton. 
An image is a natural way of representing this information with the spatial distribution represented by the arrangement of pixel values and the channels of the image representing properties such as charged particle $p_{T}$ in the pixel region.

\subsubsection{Formulation}
The image formulation used in this thesis is of two three-channel images stacked in the channels dimension to produce a $n\times{}n\times{}6$ dijet image.
The three channels are the following: $p_T$ deposition of charged candidates, $p_T$ deposition of neutral candidates, and particle multiplicity.
The space that the pixels correspond to is the space of particle displacements in pseudorapidity and azimuthal angle from the jet axis $(\Delta\eta,\Delta\phi)$,
\begin{equation}
    \begin{split}
        \Delta\eta =& \eta_{p} - \eta_{j} \\ 
        \Delta\phi =& \phi_{p} - \phi_{j} \\
    \end{split}
    \label{eq:event_categorisation:pixel_coords}
\end{equation}
where subscript $p$ denotes a constituent particle and $j$ denotes the jet. 
The pixels themselves are not a recilinear grid in $(\Delta\eta,\Delta\phi)$, but are evenly-spaced in the polar coordinates 
\begin{equation}
    \begin{split}
        \Delta{R} =& \sqrt{\Delta\eta^2 + \Delta\phi^2} \\
        \varphi   =& \mathrm{tan}(\Delta\phi/\Delta\eta) \\
    \end{split}
    \label{eq:event_categorisation:pixel_coords}
\end{equation}
These have been rotated by half a pixel in $\varphi$ so that the $(\Delta\eta,\Delta\phi)$ axes line up with the centres of a row of pixels rather than the boundary between them. 
Finally, the images are normalised such that the sum of the $p_T$-based channels equal one, and the sum of the individual multiplicity channels equal one. 
The image formulation is summarised in Figure \ref{fig:event_categorisation:jet_image}.

These images are different in their formulation and behaviour compared to a typical image.
Firstly, they are sparse with only a fraction of the pixels ever non-zero in any one image. 
Secondly, assumptions about local correlations between pixels do not apply: two adjacent red pixels would mean two adjacent particles. Max pooling will simply pick the higher valued pixel during downsampling and information about the second particle will be lost. 
Thirdly, in the rectilinear image which is seen by the network (bottom right of Figure \ref{fig:event_categorisation:jet_image}) there is a periodic boundary condition where the top pixels wrap around to the bottom ones. When convolution operations are performed on these images the padding must be periodic in the vertical direction ($\varphi$ direction).

\newpage
\begin{figure}[h!]
    \includegraphics[width=\textwidth]{figures/event_selection/jet_diagram_RGB.pdf}
    \begin{center}
        \includegraphics[width=0.49\textwidth]{figures/event_selection/full_image_polar.pdf}
        \includegraphics[width=0.49\textwidth]{figures/event_selection/full_image_rect.pdf}
    \end{center}
    \caption{\textbf{Top:} 
             construction of single-jet image from jet constituents. Arrows correspond to individial PF candidates where red arrows are charged, green are neutral and the opacity corresponds to $p_{T}$.
             The red channel measures charged candidate $p_T$ deposition in each pixel, 
             the green channel is neutral charged candidates $p_T$ deposition, 
             and the blue channel is the number of candidates in each pixel (multiplicity). 
             Multiplicity channel is drawn separately so the charged and neutral channels can be seen clearly. Black pixels are lightened to show coloured pixels more clearly.\\
             \textbf{Bottom:} 
             the final image with all the channels together.}
    \label{fig:event_categorisation:jet_image}
\end{figure}


\subsubsection{Image Dataset}
Mean images and decompositions for structure, coarse structure of forward regions
\begin{figure}[h!]
    \includegraphics[width=0.75\textwidth]{figures/event_selection/mean_vbf_PS_uw.pdf}
    \includegraphics[width=0.75\textwidth]{figures/event_selection/mean_ggh_PS_uw.pdf}
    \includegraphics[width=0.75\textwidth]{figures/event_selection/mean_bkg_PS_uw.pdf}
    \caption{Mean images}
    \label{fig:event_categorisation:mean_jet_image}
\end{figure}





\subsection{Dense CNN Model}

\subsubsection{Model Design}
The overall structure of the model can be considered to built from three main parts:
\begin{itemize}[leftmargin=.5in,noitemsep]
    \item \textbf{Convolutional section} for learning dijet substructure features from dijet images.
    \item \textbf{Merge section} for processing and integrating engineered kinematic features with learned features from the convolutional section.
    \item \textbf{Main discriminant} fully-connected layers for integrating all information and producing the class logits.
\end{itemize}

The convolutional section consists of a `spread layer' followed by three dense blocks each of which are followed by transition units.

The spread layer is a depthwise convolution layer which produces $N$-many featuremaps for each channel where the filters do not mix the image channels. 
For each channel's associated feature maps half of them have their values evenly permuted in the vertical direction, this corresponds to a rotation by $\pi$ in the polar image.
The function of this layer is to spread out the sparse image into a collection of featuremaps which correspond to simple local spatial configurations of pixels such as radial or angular bands of deposition. 
The interleaved rotations of this layer's output featuremaps allows for the comparison of pixels opposite each other around the jet axis much earlier in the network.  
This layer gives two hyperparameters to the model: the filter size and the number of features per input channel.

The dense blocks construct increasingly higher-level featuremaps, and the transition units will combine feature maps for feature reduction as well as downsampling with average pooling to avoid information loss associated with max pooling mentioned before.  
The structure of each of these parts is tuneable, and therefore gives another twelve hyperparameters to the model: three from each dense block and one from each transition unit. 

The merge section consists of a set of fully-connected layers with the first one after the initial input a different size to the others, this is then concatenated with the output of the convolutional part.
The function of this section is to embed the engineered features in a higher-dimensional space, form them into a vector the same size as the convolutional section output, and then combine them together with the jet structure features. This section has three hyperparameters: the size of the hidden layers, the number of layers, and the size of the first hidden layer relative to the others. 


The main discriminant consists of a sequence of fully-connected layers which take the full vector of concatenated features as input and produce three class logits which correspond to the VBF, ggH, and background process classes. 
These logits are then mapped to class probabilities by a softmax function, the VBF class probability is then used to define tag categories. 






\subsubsection{Loss}
(Weights and their interpretation as cost)
(Cost sensitivity: intra class and inter class costs)

Cost explaination, how the previous one tried to do costing with the ggH being left out of the combined BDT training. 

The loss function is a cost-sensitive version of cross entropy from (ref)
\begin{equation}
    L_i = -\log\left(\frac{\xi_{pp}e^{o^{i}_{p}}}{\sum_{j=0}^{3}\xi_{pj}e^{o^{i}_{j}}}\right)
\end{equation} 
where $i$ enumerates the events of the minibatch, $o_j$ is the logit of class $j$, $p$ is the true class index of the event, and $\xi$ is the cost matrix which encodes misclassification costs
\begin{equation}
    \xi = \begin{pmatrix}
        c_{\mathrm{BG}} & c_{\mathrm{BG}/\mathrm{ggH}} & c_{\mathrm{BG}/\mathrm{VBF}} \\
        c_{\mathrm{BG}/\mathrm{ggH}} & c_{\mathrm{ggH}} & c_{\mathrm{ggH}/\mathrm{VBF}} \\
        c_{\mathrm{BG}/\mathrm{VBF}} & c_{\mathrm{VBF}/\mathrm{ggH}} & c_{\mathrm{VBF}} \\
    \end{pmatrix}
\end{equation}
where each element $c$ is a real number belonging to the interval $(0,1]$. A cost of zero assigns

The loss over the entire minibatch is expressed as a weighted sum of events,
\begin{equation}
    L = \frac{1}{\sum_{j=0}^{N}w_{j}}\sum_{i=0}^{N}w_{i}L_{i}
\end{equation} 
where $N$ is the number of elements in the minibatch, and $w_i$ is the weight of event $i$.

These event weights are handled as follows


\subsubsection{Regularization}
Now control the model capactity with regularisation. 
(The regularisation)
The model is regularised with $L_2$ throughout, but with separate values for the different sections. There is also dropout applied to the fully-connected layers during training. 
Dropout is not applied to the convolutional part. 

The loss function used is a weighted version of cross entropy where the loss over the minibatch is a weighted mean. For each batch the optimiser will descend the weighted mean of the gradients for each example in the minibatch. The weights are normalised per class so each class has the same total weight, but the shape of their distribution is preserved.  



Dropout in fc layers dropout in conv

Proper implementation of weight decay for adam optimisation

Gradient clipping is applied to the gradient updates. This is crucial. Cliff edges and sparsity.



\subsubsection{Data Preprocessing and Training}

Training is done in two steps: image only and then the rest

The optimisation uses  with Nesterov momentum and learning rate decay. This is found (ref) to give better generalisation than algorithms with adaptive per-parameter learning rates such as Adam. 

All of the input features are preprocessed with mean-subtraction and division by their standard deviation. The means and standard deviations are calculated from the training set and these values are applied to the validation and test sets as well as the training set. 
The the training data are augmented by randomly reflecting the images in the $\Delta\eta$ and $\Delta\phi$ directions during training. 



\subsubsection{Neural Architecture Search}
Network architecture optimisation

The final result


\subsubsection{Final Model}
(performance and final design after hp opt)
The performance of the final model with comparison to the legacy

\subsection{Model Performance}
We would like to figure out what sort of features the convolutional section is forming. There are three approaches we will explore here: maximally activating images from the dataset, image occlusion studies, and the production of maximally activating images using stochastic gradient descent (deep dreams).  

\subsection{Model Interpretation}
We would like to figure out what sort of features the convolutional section is forming. There are three approaches we will explore here: maximally activating images from the dataset, image occlusion studies, and the production of maximally activating images using stochastic gradient descent (deep dreams).  

\subsubsection{Feature Visualisation}
We use SGD to optimise the input image with respect to the logits. We observe colour connection in vbf, more circular ggh with charge, etc.
We should note that doing this for an image recognition network such as Inception will produce a carpet of mutant dogs so this is probably similar. That's why they're non-sparse. 
\begin{figure}[h!]
    \includegraphics[width=0.8\textwidth]{figures/event_selection/nonnorm_logits0_v2.pdf}
    \includegraphics[width=0.8\textwidth]{figures/event_selection/nonnorm_logits1_v2.pdf}
    \caption{Generated images which maximally activate the output neuron (logit) for the two classes the convolutional part is trained over: ggH (top) and VBF (bottom).
             These images are normalised such that the \pt-based channels sum to unity and the multiplicity channel sums to unity for each jet image.}
    \label{fig:event_categorisation:mean_jet_image}
\end{figure}

\subsubsection{Maximally-Activating images}
Looking at images we can see that the most activating for ggh and vbf resemble the feature visualisations with SGD. 


\subsubsection{Front Filters and Low-level Features}
Front filters are the first features it builds. These are low-level arrangements of particles and it's isolating bands and spokes as well as performing small transformations and smearing. 
\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=0.32\textwidth]{figures/event_selection/front_filters_channel_0.pdf}
        \includegraphics[width=0.32\textwidth]{figures/event_selection/front_filters_channel_1.pdf}
        \includegraphics[width=0.32\textwidth]{figures/event_selection/front_filters_channel_2.pdf}
    \end{center}
    \begin{center}
        \includegraphics[width=0.32\textwidth]{figures/event_selection/front_filters_channel_3.pdf}
        \includegraphics[width=0.32\textwidth]{figures/event_selection/front_filters_channel_4.pdf}
        \includegraphics[width=0.32\textwidth]{figures/event_selection/front_filters_channel_5.pdf}
    \end{center}
    \caption{}
    \label{fig:event_categorisation:front_filters}
\end{figure}

\subsubsection{Going Deeper into the Network}
Activation visualisation deeper inside the network but it's harder to interpret

\subsubsection{Pseudorapidity Inference}
Training with pseudorapidity in the engineered features leads to no increase in discrimination power. 

\subsubsection{Conclusion}
We see that it has picked up on jet substructure, and also a small amount of kinematic information by determining detector pseudorapidity region. 




\subsection{Validation}
Zee validation of the score plus the data/sim training to tell the images apart and feature vis of the differences. Comparison to BDT-based tag. 




\subsection{Categorisation}
Categorisation re-optimised for the dense CNN model tag.





\section{Untagged}
If the candidate does not receive a tag it is conisdered for the untagged categories which make a selection diphoton BDT score. These categories mostly consist of gluon fusion events. 


